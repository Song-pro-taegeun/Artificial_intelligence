{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfT90Gn2KYEypS6l7+wa3U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1XLAeVNhOjxn","executionInfo":{"status":"ok","timestamp":1696331444353,"user_tz":-540,"elapsed":3,"user":{"displayName":"송태근","userId":"08340371274599902325"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # 모든 이미지 파일들을 읽고, 정렬하여\n","        # 이미지와 분할 마스크 정렬을 확인합니다\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # 이미지와 마스크를 읽어옵니다\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n","        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n","        mask = Image.open(mask_path)\n","        # numpy 배열을 PIL 이미지로 변환합니다\n","        mask = np.array(mask)\n","        # 인스턴스들은 다른 색들로 인코딩 되어 있습니다.\n","        obj_ids = np.unique(mask)\n","        # 첫번째 id 는 배경이라 제거합니다\n","        obj_ids = obj_ids[1:]\n","\n","        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # 각 마스크의 바운딩 박스 좌표를 얻습니다\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # 모든 것을 torch.Tensor 타입으로 변환합니다\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"metadata":{"id":"FXQU3llVO6KW","executionInfo":{"status":"ok","timestamp":1696331458181,"user_tz":-540,"elapsed":5800,"user":{"displayName":"송태근","userId":"08340371274599902325"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# COCO로 미리 학습된 모델 읽기\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","# 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의합니다\n","num_classes = 2  # 1 클래스(사람) + 배경\n","# 분류기에서 사용할 입력 특징의 차원 정보를 얻습니다\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체합니다\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mc-bVu0HO6Nl","executionInfo":{"status":"ok","timestamp":1696331480122,"user_tz":-540,"elapsed":6246,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"6c55a769-60a3-4ed4-c7a0-f2c8b21fafa7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:01<00:00, 96.4MB/s]\n"]}]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n","backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n","# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n","backbone.out_channels = 1280\n","\n","# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n","# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n","# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n","\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n","# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n","# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n","# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# 조각들을 Faster RCNN 모델로 합칩니다.\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EppSSEr3O6Qd","executionInfo":{"status":"ok","timestamp":1696331502443,"user_tz":-540,"elapsed":4744,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"1aa3a7a2-cf28-46bb-ee27-0332b00f5d83"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 17.2MB/s]\n"]}]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","    # 분류를 위한 입력 특징 차원을 얻습니다\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # 마스크 예측기를 새로운 것으로 바꿉니다\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"metadata":{"id":"9all-eNOO6TN","executionInfo":{"status":"ok","timestamp":1696331511570,"user_tz":-540,"elapsed":2,"user":{"displayName":"송태근","userId":"08340371274599902325"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["pip install transforms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6a1W4SIPuQ2","executionInfo":{"status":"ok","timestamp":1696331664637,"user_tz":-540,"elapsed":8182,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"576a6bfe-0ffb-4ff6-d30f-2578d4ba95bc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transforms\n","  Downloading transforms-0.1.tar.gz (29 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: transforms\n","  Building wheel for transforms (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transforms: filename=transforms-0.1-py3-none-any.whl size=39339 sha256=22cb552a5fc948703677dd60b5c859f2c237180ad7f1ca4594cbe382c0a87b6a\n","  Stored in directory: /root/.cache/pip/wheels/4b/02/1f/803fdcba065de0f79a6c7b7dca7c5e671feed4bf34c3264166\n","Successfully built transforms\n","Installing collected packages: transforms\n","Successfully installed transforms-0.1\n"]}]},{"cell_type":"code","source":["# import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.PILToTensor())\n","    transforms.append(T.ConvertImageDtype(torch.float))\n","    if train:\n","        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"metadata":{"id":"0MHKruqtO6WF","executionInfo":{"status":"ok","timestamp":1696331775674,"user_tz":-540,"elapsed":2,"user":{"displayName":"송태근","userId":"08340371274599902325"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n"," dataset, batch_size=2, shuffle=True, num_workers=4,\n"," collate_fn=utils.collate_fn)\n","# 학습 시\n","images,targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","output = model(images,targets)   # Returns losses and detections\n","# 추론 시\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)           # Returns predictions"],"metadata":{"id":"21lKqF-DQiMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","\n","def main():\n","    # 학습을 GPU로 진행하되 GPU가 가용하지 않으면 CPU로 합니다\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    # 우리 데이터셋은 두 개의 클래스만 가집니다 - 배경과 사람\n","    num_classes = 2\n","    # 데이터셋과 정의된 변환들을 사용합니다\n","    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","    # 데이터셋을 학습용과 테스트용으로 나눕니다(역자주: 여기서는 전체의 50개를 테스트에, 나머지를 학습에 사용합니다)\n","    indices = torch.randperm(len(dataset)).tolist()\n","    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","    # 데이터 로더를 학습용과 검증용으로 정의합니다\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=2, shuffle=True, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    data_loader_test = torch.utils.data.DataLoader(\n","        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    # 도움 함수를 이용해 모델을 가져옵니다\n","    model = get_model_instance_segmentation(num_classes)\n","\n","    # 모델을 GPU나 CPU로 옮깁니다\n","    model.to(device)\n","\n","    # 옵티마이저(Optimizer)를 만듭니다\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)\n","    # 학습률 스케쥴러를 만듭니다\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                   step_size=3,\n","                                                   gamma=0.1)\n","\n","    # 10 에포크만큼 학습해봅시다\n","    num_epochs = 10\n","\n","    for epoch in range(num_epochs):\n","        # 1 에포크동안 학습하고, 10회 마다 출력합니다\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","        # 학습률을 업데이트 합니다\n","        lr_scheduler.step()\n","        # 테스트 데이터셋에서 평가를 합니다\n","        evaluate(model, data_loader_test, device=device)\n","\n","    print(\"That's it!\")"],"metadata":{"id":"Nlwovq9vQjpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G0wnb9c1O6eP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NpO9t4hQO6hJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c2aTlaVZO6j2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qV0neu2HO6mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dGhbHCVxO6pw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GC4r5a9FO6sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MX776tiLO6vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oD5F0O3tO6yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tWqS3wk3O61f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S21r306pO64Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_Qey2MlsO66-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mNXuTgjmO692"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TalJbgUvO7Ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GPCHMNrZO7DE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"07XfmDiLO7Fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fpykXPbJO7Io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CyJXhI4dO7LX"},"execution_count":null,"outputs":[]}]}