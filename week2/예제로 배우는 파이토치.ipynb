{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSRJNPrIl/oygGmUV1To1p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import math\n","\n","# 무작위로 입력과 출력 데이터를 생성합니다\n","x = np.linspace(-math.pi, math.pi, 2000)\n","y = np.sin(x)\n","\n","# 무작위로 가중치를 초기화합니다\n","a = np.random.randn()\n","b = np.random.randn()\n","c = np.random.randn()\n","d = np.random.randn()\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 예측값 y를 계산합니다\n","    # y = a + b x + c x^2 + d x^3\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 손실(loss)을 계산하고 출력합니다\n","    loss = np.square(y_pred - y).sum()\n","    if t % 100 == 99:\n","        print(t, loss)\n","\n","    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = (grad_y_pred * x ** 3).sum()\n","\n","    # 가중치를 갱신합니다.\n","    a -= learning_rate * grad_a\n","    b -= learning_rate * grad_b\n","    c -= learning_rate * grad_c\n","    d -= learning_rate * grad_d\n","\n","print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zXuE0I4LTl2","executionInfo":{"status":"ok","timestamp":1696330503237,"user_tz":-540,"elapsed":6,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"bad1d687-a7ab-4799-8a59-9a3a98254343"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["99 71.21223531017445\n","199 50.54963008164276\n","299 36.7497959699431\n","399 27.526123237776957\n","499 21.35724300334641\n","599 17.22872875766928\n","699 14.463831607638397\n","799 12.610834198148467\n","899 11.368054157257744\n","999 10.533892192985743\n","1099 9.973546338115812\n","1199 9.596821251855507\n","1299 9.343326894786605\n","1399 9.172600767425187\n","1499 9.057512279246875\n","1599 8.979856125459824\n","1699 8.927406377497187\n","1799 8.891945834340358\n","1899 8.867946891431329\n","1999 8.851687914747838\n","Result: y = 0.004014444439338235 + 0.8523785084854779 x + -0.0006925584664296565 x^2 + -0.09270988493295607 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","import torch\n","import math\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n","\n","# 무작위로 입력과 출력 데이터를 생성합니다\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# 무작위로 가중치를 초기화합니다\n","a = torch.randn((), device=device, dtype=dtype)\n","b = torch.randn((), device=device, dtype=dtype)\n","c = torch.randn((), device=device, dtype=dtype)\n","d = torch.randn((), device=device, dtype=dtype)\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 예측값 y를 계산합니다\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 손실(loss)을 계산하고 출력합니다\n","    loss = (y_pred - y).pow(2).sum().item()\n","    if t % 100 == 99:\n","        print(t, loss)\n","\n","    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = (grad_y_pred * x ** 3).sum()\n","\n","    # 가중치를 갱신합니다.\n","    a -= learning_rate * grad_a\n","    b -= learning_rate * grad_b\n","    c -= learning_rate * grad_c\n","    d -= learning_rate * grad_d\n","\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2dkM74gLWrO","executionInfo":{"status":"ok","timestamp":1696330512539,"user_tz":-540,"elapsed":492,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"33789f11-70ff-4152-efca-87c324a98b9d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["99 1426.365234375\n","199 990.1875610351562\n","299 688.9004516601562\n","399 480.56695556640625\n","499 336.361572265625\n","599 236.44534301757812\n","699 167.14849853515625\n","799 119.04209899902344\n","899 85.61544036865234\n","999 62.36848831176758\n","1099 46.187103271484375\n","1199 34.91434860229492\n","1299 27.054973602294922\n","1399 21.57115936279297\n","1499 17.742040634155273\n","1599 15.066404342651367\n","1699 13.195489883422852\n","1799 11.886428833007812\n","1899 10.969921112060547\n","1999 10.327845573425293\n","Result: y = -0.03859560191631317 + 0.8698238134384155 x + 0.006658381782472134 x^2 + -0.09519132226705551 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import torch\n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n","# 계산할 필요가 없음을 나타냅니다.\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n","# y = a + b x + c x^2 + d x^3\n","# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n","# 있음을 나타냅니다.\n","a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 텐서들간의 연산을 사용하여 손실(loss)을 계산하고 출력합니다.\n","    # 이 때 손실은 (1,) shape을 갖는 텐서입니다.\n","    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는\n","    # 모든 텐서들에 대한 손실의 변화도를 계산합니다.\n","    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n","    # 갖는 텐서가 됩니다.\n","    loss.backward()\n","\n","    # 경사하강법(gradient descent)을 사용하여 가중치를 직접 갱신합니다.\n","    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n","    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= learning_rate * d.grad\n","\n","        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BF-RYYeZLbku","executionInfo":{"status":"ok","timestamp":1696330533580,"user_tz":-540,"elapsed":1215,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"b6feafe0-7cf3-4a8d-aebc-45f01aa9d2ed"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["99 178.384521484375\n","199 124.21914672851562\n","299 87.44573974609375\n","399 62.45159149169922\n","499 45.44423294067383\n","599 33.85861587524414\n","699 25.957416534423828\n","799 20.562700271606445\n","899 16.8751277923584\n","999 14.351578712463379\n","1099 12.622618675231934\n","1199 11.436726570129395\n","1299 10.622379302978516\n","1399 10.062545776367188\n","1499 9.677248001098633\n","1599 9.411781311035156\n","1699 9.228667259216309\n","1799 9.102231979370117\n","1899 9.014839172363281\n","1999 8.954368591308594\n","Result: y = -0.010530387051403522 + 0.8627511858940125 x + 0.0018166661029681563 x^2 + -0.09418530762195587 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import torch\n","import math\n","\n","\n","class LegendrePolynomial3(torch.autograd.Function):\n","    \"\"\"\n","    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n","    텐서 연산을 하는 순전파 단계와 역전파 단계를 구현해보겠습니다.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        \"\"\"\n","        순전파 단계에서는 입력을 갖는 텐서를 받아 출력을 갖는 텐서를 반환합니다.\n","        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용합니다.\n","        ctx.save_for_backward 메소드를 사용하여 역전파 단계에서 사용할 어떤 객체도\n","        저장(cache)해 둘 수 있습니다.\n","        \"\"\"\n","        ctx.save_for_backward(input)\n","        return 0.5 * (5 * input ** 3 - 3 * input)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고,\n","        입력에 대한 손실의 변화도를 계산해야 합니다.\n","        \"\"\"\n","        input, = ctx.saved_tensors\n","        return grad_output * 1.5 * (5 * input ** 2 - 1)\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할\n","# 필요가 없음을 나타냅니다.\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n","# y = a + b * P3(c + d * x)\n","# 이 가중치들이 수렴(convergence)하기 위해서는 정답으로부터 너무 멀리 떨어지지 않은 값으로\n","# 초기화가 되어야 합니다.\n","# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n","# 있음을 나타냅니다.\n","a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n","c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 5e-6\n","for t in range(2000):\n","    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n","    # 여기에 'P3'라고 이름을 붙였습니다.\n","    P3 = LegendrePolynomial3.apply\n","\n","    # 순전파 단계: 연산을 하여 예측값 y를 계산합니다;\n","    # 사용자 정의 autograd 연산을 사용하여 P3를 계산합니다.\n","    y_pred = a + b * P3(c + d * x)\n","\n","    # 손실을 계산하고 출력합니다.\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # autograd를 사용하여 역전파 단계를 계산합니다.\n","    loss.backward()\n","\n","    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= learning_rate * d.grad\n","\n","        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5W7UPZzLf5W","executionInfo":{"status":"ok","timestamp":1696330550398,"user_tz":-540,"elapsed":838,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"389d849d-2ced-49f0-9074-f889cf8bf155"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["99 209.95834350585938\n","199 144.66018676757812\n","299 100.70249938964844\n","399 71.03519439697266\n","499 50.978511810302734\n","599 37.403133392333984\n","699 28.206867218017578\n","799 21.97318458557129\n","899 17.7457275390625\n","999 14.877889633178711\n","1099 12.93176555633545\n","1199 11.610918045043945\n","1299 10.71425724029541\n","1399 10.10548210144043\n","1499 9.692105293273926\n","1599 9.411375999450684\n","1699 9.220745086669922\n","1799 9.091285705566406\n","1899 9.003361701965332\n","1999 8.943641662597656\n","Result: y = -6.71270206087371e-10 + -2.208526849746704 * P3(-3.392665037793563e-10 + 0.2554861009120941 x)\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import torch\n","import math\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","x = torch.linspace(-math.pi, math.pi, 2000)\n","y = torch.sin(x)\n","\n","# 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주할 수 있습니다.\n","# (x, x^2, x^3)를 위한 텐서를 준비합니다.\n","p = torch.tensor([1, 2, 3])\n","xx = x.unsqueeze(-1).pow(p)\n","\n","# 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,\n","# 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻습니다.\n","\n","# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n","# nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여\n","# 출력을 생성합니다. 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터\n","# 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n","# Flatten 계층은 선형 계층의 출력을 `y` 의 shape과 맞도록(match) 1D 텐서로 폅니다(flatten).\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(3, 1),\n","    torch.nn.Flatten(0, 1)\n",")\n","\n","# 또한 nn 패키지에는 주로 사용되는 손실 함수(loss function)들에 대한 정의도 포함되어 있습니다;\n","# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","\n","    # 순전파 단계: x를 모델에 전달하여 예측값 y를 계산합니다. Module 객체는 __call__ 연산자를\n","    # 덮어써서(override) 함수처럼 호출할 수 있도록 합니다. 이렇게 함으로써 입력 데이터의 텐서를 Module에 전달하여\n","    # 출력 데이터의 텐서를 생성합니다.\n","    y_pred = model(xx)\n","\n","    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 텐서들을 전달하고,\n","    # 손실 함수는 손실(loss)을 갖는 텐서를 반환합니다.\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 만듭니다.\n","    model.zero_grad()\n","\n","    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산합니다.\n","    # 내부적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,\n","    # 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됩니다.\n","    loss.backward()\n","\n","    # 경사하강법을 사용하여 가중치를 갱신합니다.\n","    # 각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있습니다.\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= learning_rate * param.grad\n","\n","# list의 첫번째 항목에 접근하는 것처럼 `model` 의 첫번째 계층(layer)에 접근할 수 있습니다.\n","linear_layer = model[0]\n","\n","# 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됩니다.\n","print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvjC543BLjT-","executionInfo":{"status":"ok","timestamp":1696330564568,"user_tz":-540,"elapsed":528,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"567912e6-974a-48e8-88ac-e46aec2d5ac0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["99 122.38597106933594\n","199 83.91636657714844\n","299 58.477848052978516\n","399 41.656211853027344\n","499 30.532623291015625\n","599 23.177001953125\n","699 18.312944412231445\n","799 15.096502304077148\n","899 12.96955680847168\n","999 11.563063621520996\n","1099 10.632986068725586\n","1199 10.01793384552002\n","1299 9.611220359802246\n","1399 9.342260360717773\n","1499 9.164410591125488\n","1599 9.046799659729004\n","1699 8.969011306762695\n","1799 8.917579650878906\n","1899 8.88357162475586\n","1999 8.861083984375\n","Result: y = 0.00025422938051633537 + 0.8503031134605408 x + -4.385918509797193e-05 x^2 + -0.09241466969251633 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import torch\n","import math\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","x = torch.linspace(-math.pi, math.pi, 2000)\n","y = torch.sin(x)\n","\n","# 입력 텐서 (x, x^2, x^3)를 준비합니다.\n","p = torch.tensor([1, 2, 3])\n","xx = x.unsqueeze(-1).pow(p)\n","\n","# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(3, 1),\n","    torch.nn.Flatten(0, 1)\n",")\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","# optim 패키지를 사용하여 모델의 가중치를 갱신할 optimizer를 정의합니다.\n","# 여기서는 RMSprop을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을 포함하고 있습니다.\n","# RMSprop 생성자의 첫번째 인자는 어떤 텐서가 갱신되어야 하는지를 알려줍니다.\n","learning_rate = 1e-3\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n","for t in range(2000):\n","    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n","    y_pred = model(xx)\n","\n","    # 손실을 계산하고 출력합니다.\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # 역전파 단계 전에, optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인) 갱신할\n","    # 변수들에 대한 모든 변화도(gradient)를 0으로 만듭니다. 이렇게 하는 이유는 기본적으로\n","    # .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기\n","    # 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.\n","    optimizer.zero_grad()\n","\n","    # 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산합니다.\n","    loss.backward()\n","\n","    # optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n","    optimizer.step()\n","\n","\n","linear_layer = model[0]\n","print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wd2vy9-MLoGd","executionInfo":{"status":"ok","timestamp":1696330585700,"user_tz":-540,"elapsed":1187,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"bd9310be-7586-47d6-d0a8-50c225d03991"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["99 4606.5107421875\n","199 2693.377685546875\n","299 1743.603271484375\n","399 1257.138427734375\n","499 970.4564208984375\n","599 745.9265747070312\n","699 558.9712524414062\n","799 407.3829650878906\n","899 287.7411193847656\n","999 196.08157348632812\n","1099 128.10397338867188\n","1199 79.36036682128906\n","1299 45.59423828125\n","1399 24.67534637451172\n","1499 13.821096420288086\n","1599 9.769662857055664\n","1699 8.946310043334961\n","1799 8.90693473815918\n","1899 8.911614418029785\n","1999 8.904224395751953\n","Result: y = -3.122891939710826e-05 + 0.8563088774681091 x + -3.1700110412202775e-05 x^2 + -0.0938289538025856 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import torch\n","import math\n","\n","\n","class Polynomial3(torch.nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        생성자에서 4개의 매개변수를 생성(instantiate)하고, 멤버 변수로 지정합니다.\n","        \"\"\"\n","        super().__init__()\n","        self.a = torch.nn.Parameter(torch.randn(()))\n","        self.b = torch.nn.Parameter(torch.randn(()))\n","        self.c = torch.nn.Parameter(torch.randn(()))\n","        self.d = torch.nn.Parameter(torch.randn(()))\n","\n","    def forward(self, x):\n","        \"\"\"\n","        순전파 함수에서는 입력 데이터의 텐서를 받고 출력 데이터의 텐서를 반환해야 합니다.\n","        텐서들 간의 임의의 연산뿐만 아니라, 생성자에서 정의한 Module을 사용할 수 있습니다.\n","        \"\"\"\n","        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n","\n","    def string(self):\n","        \"\"\"\n","        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.\n","        \"\"\"\n","        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n","\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","x = torch.linspace(-math.pi, math.pi, 2000)\n","y = torch.sin(x)\n","\n","# 위에서 정의한 클래스로 모델을 생성합니다.\n","model = Polynomial3()\n","\n","# 손실 함수와 optimizer를 생성합니다. SGD 생성자에 model.paramaters()를 호출해주면\n","# 모델의 멤버 학습 가능한 (torch.nn.Parameter로 정의된) 매개변수들이 포함됩니다.\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n","for t in range(2000):\n","    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n","    y_pred = model(x)\n","\n","    # 손실을 계산하고 출력합니다.\n","    loss = criterion(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","print(f'Result: {model.string()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlAL65SHLqNm","executionInfo":{"status":"ok","timestamp":1696330604802,"user_tz":-540,"elapsed":1562,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"a0aae2a8-928e-408e-8b3e-65c9c03d9c65"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["99 3357.31201171875\n","199 2236.23681640625\n","299 1491.0592041015625\n","399 995.5742797851562\n","499 665.9981689453125\n","599 446.6954040527344\n","699 300.7115478515625\n","799 203.49349975585938\n","899 138.72250366210938\n","999 95.54901123046875\n","1099 66.75716400146484\n","1199 47.54658126831055\n","1299 34.72185516357422\n","1399 26.15528106689453\n","1499 20.429563522338867\n","1599 16.600322723388672\n","1699 14.037714958190918\n","1799 12.321568489074707\n","1899 11.171503067016602\n","1999 10.400165557861328\n","Result: y = -0.02115638554096222 + 0.8232990503311157 x + 0.003649829188361764 x^2 + -0.08857358247041702 x^3\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import random\n","import torch\n","import math\n","\n","\n","class DynamicNet(torch.nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        생성자에서 5개의 매개변수를 생성(instantiate)하고 멤버 변수로 지정합니다.\n","        \"\"\"\n","        super().__init__()\n","        self.a = torch.nn.Parameter(torch.randn(()))\n","        self.b = torch.nn.Parameter(torch.randn(()))\n","        self.c = torch.nn.Parameter(torch.randn(()))\n","        self.d = torch.nn.Parameter(torch.randn(()))\n","        self.e = torch.nn.Parameter(torch.randn(()))\n","\n","    def forward(self, x):\n","        \"\"\"\n","        모델의 순전파 단계에서는 무작위로 4, 5 중 하나를 선택한 뒤 매개변수 e를 재사용하여\n","        이 차수들의의 기여도(contribution)를 계산합니다.\n","\n","        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를 정의할 때\n","        반복문이나 조건문과 같은 일반적인 Python 제어-흐름 연산자를 사용할 수 있습니다.\n","\n","        여기에서 연산 그래프를 정의할 때 동일한 매개변수를 여러번 사용하는 것이 완벽히 안전하다는\n","        것을 알 수 있습니다.\n","        \"\"\"\n","        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n","        for exp in range(4, random.randint(4, 6)):\n","            y = y + self.e * x ** exp\n","        return y\n","\n","    def string(self):\n","        \"\"\"\n","        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.\n","        \"\"\"\n","        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n","\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","x = torch.linspace(-math.pi, math.pi, 2000)\n","y = torch.sin(x)\n","\n","# 위에서 정의한 클래스로 모델을 생성합니다.\n","model = DynamicNet()\n","\n","# 손실 함수와 optimizer를 생성합니다. 이 이상한 모델을 순수한 확률적 경사하강법(SGD; Stochastic Gradient Descent)으로\n","# 학습하는 것은 어려우므로, 모멘텀(momentum)을 사용합니다.\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n","for t in range(30000):\n","    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n","    y_pred = model(x)\n","\n","    # 손실을 계산하고 출력합니다.\n","    loss = criterion(y_pred, y)\n","    if t % 2000 == 1999:\n","        print(t, loss.item())\n","\n","    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","print(f'Result: {model.string()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Asz8S48nLw3G","executionInfo":{"status":"ok","timestamp":1696330634922,"user_tz":-540,"elapsed":16310,"user":{"displayName":"송태근","userId":"08340371274599902325"}},"outputId":"6d3eff36-723f-4110-c141-24eec0b3d157"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["1999 803.21044921875\n","3999 355.232177734375\n","5999 168.86412048339844\n","7999 79.52960968017578\n","9999 39.42338180541992\n","11999 22.58770751953125\n","13999 14.880983352661133\n","15999 11.47750473022461\n","17999 9.995108604431152\n","19999 9.209657669067383\n","21999 8.878348350524902\n","23999 8.930108070373535\n","25999 8.628533363342285\n","27999 8.622932434082031\n","29999 8.840566635131836\n","Result: y = 0.0017343021463602781 + 0.8548154234886169 x + -0.0007768609211780131 x^2 + -0.09322553873062134 x^3 + 0.00010002525959862396 x^4 ? + 0.00010002525959862396 x^5 ?\n"]}]}]}